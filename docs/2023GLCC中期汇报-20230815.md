## 2023GLCC中期汇报

时间：2023.08.15

CCF GitLink编程夏令营（GitLink Code Camp，简称GLCC），是由中国计算机学会（CCF）主办，GitLink社区和CCF开源发展委员会（CCF ODC）共同举办的面向全国高校学生的暑期开源项目实习计划。

我是 67，我的课题题目是 "PyLayer 功能支持动转静"，非常感谢 dalao 导师对我的悉心指导，下面我将从课题背景、课题解决思路、课题进展和成果、下一步工作计划、自我总结五个方面向您做一一介绍，恳请您批评指导。



## 课题背景

在深度学习模型构建上，飞桨框架支持动态图编程和静态图编程两种方式，其代码编写和执行方式均存在差异。动态图编程体验更佳、更易调试，但是因为采用 Python 实时执行的方式，开销较大，在性能方面与 C++ 有一定差距；静态图调试难度大，但是将前端 Python 编写的神经网络预定义为 Program 描述，转到 C++ 端重新解析执行，脱离了 Python 依赖，往往执行性能更佳，并且预先拥有完整网络结构也更利于全局优化。Paddle 目前提供 `to_static` 来完成动态图编程的模型到静态图模型的转换。

飞桨的动态图 PyLayer 向用户提供了一种高度灵活且便利的自定义网络层前、反向计算的机制，比如存在一些用户自定义的计算逻辑，无法通过飞桨 现有的某个 API 或某些 API 组合实现，故可以借助 PyLayer 来实现。但是目前当动态图模型中包含 PyLayer 接口功能的使用时，暂不支持 @to_static 装饰以生成对应的静态图 Program。

本项目要求飞桨的动态图中的 PyLayer 机制能够与飞桨的动转静 (to_static) 互通，支持模型中 PyLayer 的自定义层能够被 @to_static 感知并正确地生成静态图 Program，进而支撑转静训练和导出推理。

**如下是一个功能支持后的流程样例：**

```python
from symbol import parameters
import paddle
from paddle.autograd import PyLayer
from paddle.jit import to_static

# Inherit from PyLayer
class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x): 
        # ctx is a context object that store some objects for backward.
        y = paddle.tanh(x)       # <------ 仅仅包含 Paddle API 的计算
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    # forward has only one output, so there is only one gradient in the input of backward.
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))   # <------ 仅仅包含 Paddle API 的计算
        # forward has only one input, so only one gradient tensor is returned.
        return grad

class SimpleNet(paddle.nn.Layer):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.linear = paddle.nn.Linear(4, 8)
    
    @to_static
    def forward(self, x):
        y = self.linear(x)
        out = cus_tanh.apply(y)
        out = paddle.mean(out)
        return out


def train(net, opt):
    batch_num = 10
    for i in range(batch_num):
        data = paddle.randn([2, 4])
        out = net(data)
        out.backward()
        opt.step()
        opt.clear_grad()
        print("loss: ", out.item())
    
    save(net)


def save(net):
    path = "simple_net"
    x_spec = paddle.static.InputSpec(shape=[-1, 4], dtype='float32', name='x')
    paddle.jit.save(net, path, input_spec=[x_spec])


if __name__ == '__main__':
    net = SimpleNet()
    sgd = paddle.optimizer.SGD(0.001, parameters=net.parameters())
    train(net, sgd)
```



### 课题解决思路

根据前期对 Paddle 中 PyLayer 实现机制和 动转静@to_static 模块的调研分析，可以将课题的解决思路划分为以下3个步骤：

1. 构建静态图的 PyLayer 算子，支持运行用户自定义的前向 block 和反向 block
2. @to_static 动转静能感知 PyLayer 模块，并对 forward 逻辑和 backward 逻辑进行转写，分别生成前向 block 和 反向 block
3.  @to_static 动转静能感知并转写和处理 PyLayerContext 的相关逻辑，完成前向和反向的变量传递：保存前向过程产生的中间变量，在反向阶段取出相关的中间变量进行运算



#### 1. 构建静态图 PyLayer OP

静态图 PyLayer 算子的期望的形态：无 kernel op，其接受用户自定义的前向 block 和反向 block，并在前向过程中运行用户自定义的前向 block；在反向 op 构建时根据 op 保存的 `blocks` 信息指定反向 op 运行的 block 为用户自定义的反向 block，从而实现正反向 block 的关联

**静态图 PyLayer OP 的前向 op maker 如下：**

```c++
class StaticPyLayerForwardOpProtoMaker
    : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput(StaticPyLayerOp::kInputs, "The input variables of the sub-block.")
        .AsDuplicable();
    AddOutput(StaticPyLayerOp::kOutputs,
              "The output variables of the sub-block.")
        .AsDuplicable();
    AddOutput(StaticPyLayerOp::kScope,
              "(std::vector<Scope*>) The scope of static pylayer block.");
    AddAttr<std::vector<framework::BlockDesc *>>(
        "blocks", "The blocks of PyLayer operator");
  }
};

```

其前向 op 持有 

* 1 个输入：
  * `Input`:  Variables 类型，为前向 block 的输入
* 2 个输出：
  * `Out`：Variables 类型，为前向 block 的输出结果
  * `Scope`：STEP_SCOPES 类型，为前向 block 运算过程中所使用的 scope，保存了前向传递给反向的中间变量信息
* 2 个属性：
  * `blocks`：BLOCKS 类型，保存前向 block 和 反向 block 的 BlockDesc，为用户构建并传入
  * `skip_eager_deletion_vars`：STRINGS 类型，指明 scope 保存的 Variables

静态图的 PyLayer 的前向 op 的执行概括为以下三个步骤：

1. 从 blocks 属性中获取前向 block 的 BlockDesc
2. 设置 ExecutionConfig 等参数，创建执行器
3. 运行



**静态图 PyLayer OP 的反向 op maker 如下：**

```c++
template <typename T>
class StaticPyLayerBackwardMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("static_pylayer_grad");
    grad_op->SetInput(StaticPyLayerOp::kInputs,
                      this->Input(StaticPyLayerOp::kInputs));
    grad_op->SetInput(framework::GradVarName(StaticPyLayerOp::kOutputs),
                      this->OutputGrad(StaticPyLayerOp::kOutputs));
    grad_op->SetInput(StaticPyLayerOp::kScope,
                      this->Output(StaticPyLayerOp::kScope));

    auto fwd_inputs = this->InputGrad(StaticPyLayerOp::kInputs, false);
    grad_op->SetOutput(framework::GradVarName(StaticPyLayerOp::kInputs),
                       fwd_inputs);

    const std::vector<framework::BlockDesc *> &blocks =
        PADDLE_GET_CONST(std::vector<framework::BlockDesc *>,
                         this->GetAttr(StaticPyLayerOp::kBlocks));
    PADDLE_ENFORCE_GT(
        blocks.size(),
        static_cast<size_t>(PyLayerBlockIndex::kBACKWARD),
        platform::errors::InvalidArgument(
            "Expect blocks contains at least 2 block, but got: %d",
            blocks.size()));
    grad_op->SetBlockAttr(
        "backward_block",
        blocks[static_cast<size_t>(PyLayerBlockIndex::kBACKWARD)]);
  }
};
```

其反向 op 持有 

* 3 个输入：
  * `Input`:  Variables 类型，前向 block 的输入，作用为得到前向输入的 op 的所有信息
  * `Out@GRAD`：Variables 类型，前向 block 输出的梯度
  * `Scope`：STEP_SCOPES 类型，为前向 block 运算过程中所使用的 scope，保存了前向传递给反向的中间变量信息
* 1 个输出：
  * `Input@GRAD`：Variables 类型，为前向 block 输入的梯度
* 1 个属性：
  * `backward_block`：BLOCK 类型，为该反向 op 运行的 BlockDesc。从前向 op 的 "blocks" 属性中获取

静态图的 PyLayer 的前向 op 的执行概括为以下三个步骤：

1. 从 backward_block 属性中获取将执行的的 BlockDesc
2. 设置 ExecutionConfig 等参数，创建执行器
3. 运行



**静态图 PyLayer op 的 python API**：

```python
def static_pylayer(forward_fn, inputs, backward_fn):
```

接收三个输入：

1. forward_fn：静态图前向函数
2. inputs：静态图前向函数的输入
3. backward_fn：静态图反向函数

静态图 PyLayer op 的 python API 使用 `StaticPyLayerBlock` 作为 block 创建的上下文管理器 ( 其作用类似于现有的 `ConditionalBlockGuard` )，分别创建正反向 block. 需要注意的是，前向 block 的创建有用户直接传入的 `inputs` 变量作为输入信息，而反向 block 创建的输入是名为 `grad_var_ins` 的临时变量，根据 “变量和变量的梯度具有相同的形状和数据类型” 这一规则，可以对 `grad_var_ins` 进行形状和数据类型的推导，其名称为 前向 block 输出名称加上后缀 "@GRAD"，如：前向 op 的输出变量名称为 "tmp_0.mean_0"，则反向 op 的输入变量名称为 "tmp_0.mean_0@GRAD"。

还需要注意的一点是，构建反向 block 会得到反向 block 的输出名称，需要将其名字重命名为符合 “前向 block 输入的梯度” 的命名规范，以便反向 block 在执行器运行时对父 block 的变量进行赋值。

最终运行静态图 PyLayer 算子得到的 Program 示例如下：

```bash
{ // block 0
    var X : LOD_TENSOR.shape(-1, 5).dtype(float32).stop_gradient(False)
    var _generated_var_0 : LOD_TENSOR.shape(-1, 5).dtype(float32).stop_gradient(False)
    var _generated_var_1 : STEP_SCOPES)
    var mean_0.tmp_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    var X@GRAD : LOD_TENSOR.shape(-1, 5).dtype(float32).stop_gradient(False)
    var _generated_var_0@GRAD : LOD_TENSOR.shape(-1, 5).dtype(float32).stop_gradient(False)
    var mean_0.tmp_0@GRAD : LOD_TENSOR.shape().dtype(float32).stop_gradient(False)
    persist var learning_rate_0 : LOD_TENSOR.shape().dtype(float32).stop_gradient(True)

    {Out=['_generated_var_0'], Scope=['_generated_var_1']} = static_pylayer(inputs={Input=['X']}, blocks = blocks[1, 2], op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['mean_0.tmp_0']} = reduce_mean(inputs={X=['_generated_var_0']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 256, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
    {Out=['mean_0.tmp_0@GRAD']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_namescope = , op_role = 257, op_role_var = [], place_type = -1, shape = [], str_value = , value = 1.0, with_quant_attr = False)
    {X@GRAD=['_generated_var_0@GRAD']} = reduce_mean_grad(inputs={Out@GRAD=['mean_0.tmp_0@GRAD'], X=['_generated_var_0']}, dim = [], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
    {Input@GRAD=['X@GRAD']} = static_pylayer_grad(inputs={Input=['X'], Out@GRAD=['_generated_var_0@GRAD'], Scope=['_generated_var_1']}, backward_block = block[2], op_device = , op_role = 1)
}
{ // block 1
    var tanh_0.tmp_0 : LOD_TENSOR.shape(-1, 5).dtype(float32).stop_gradient(False)

    {Out=['tanh_0.tmp_0']} = tanh(inputs={X=['X']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
    {Out=['_generated_var_0']} = assign(inputs={X=['tanh_0.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], with_quant_attr = False)
}
{ // block 2
    var X@GRAD : LOD_TENSOR.shape(-1, 5).dtype(float32).stop_gradient(False)

    {Out=['X@GRAD']} = scale(inputs={ScaleTensor=[], X=['_generated_var_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], scale = 2.0, with_quant_attr = False)
}
```



#### 2. 使 @to_static 动转静能感知 PyLayer 模块

此处的 "感知" 有三层含义：

1. ast 模块能感知到`PyLayer`，并使用 `convert_call` 对 PyLayer 进行转写
2. 对 PyLayer 的 forward 和 backward 方法进行静态图转写
3. 根据 forward 和 backward 的逻辑生成前向 block 和反向 block



**ast 模块能在感知到 `PyLayer`**，并使用 `convert_call` 进行转写，进而转写后的 PyLayer.apply 最终应该会调用 StaticPyLayer.apply

目前的解决思路是能感知 PyLayer 的 apply 调用，即 `PyLayer.apply` , 只需要在 `convert_call_func.py `的 `convert_call` 函数里对函数的 _\_self\_\_  属性进行类型判断是否为 `PyLayerMeta` 即可，若是则 **对 PyLayer 的 forward 和 backward 方法进行静态图转写**, 并返回持有这两个已转写方法的 StaticPyLayer。StaticPyLayer.apply 最终调用静态图 PyLayer 算子提供的 python API，这个 python API 会对 forward 和 backward 逻辑分别生成前向和反向的block



#### 3.  @to_static 动转静能感知并转写和处理 PyLayerContext 的相关逻辑

`PyLayerContext` 能够协助 `PyLayer` 完成特定的功能，比如 `save_for_backward` 函数可以保存 backward 需要的中间变量；`saved_tensor` 函数可以获取被 `save_for_backward` 保存的中间变量。对于本次项目，我们只需要考虑 `PyLayerContext` 上述的两个功能。

**感知 PyLayerContext**

在 "2. 使 @to_static 动转静能感知 PyLayer 模块" 中，PyLayer 会被转写为 StaticPyLayer，在 StaticPyLayer 中实例化一个 StaticPyLayerContext 类，这个类同样具有 `save_for_backward` 函数和 `saved_tensor`  函数。在 StaticPyLayer 中将StaticPyLayerContext 与转写的 forward 和 backward 方法绑定，进而使得 `ctx.save_for_backward` 和 `ctx.saved_tensor` 的 ctx 指向的是 StaticPyLayerContext。如此，我们可以定义 `save_for_backward` 和 `saved_tensor`  内部的逻辑进行处理，这样便达到了“感知 PyLayerContext” 的目的



**处理 PyLayerContext 的逻辑**

`save_for_backward` 作用是保存 backward 需要的中间变量，在动转静中， `save_for_backward` 可以记录下保存的中间变量的变量名字，在创建静态图 pylayer 算子时将变量名字作为属性传递给静态图 pylayer op，使其运行期执行器执行前向 block 后不删除 scope 里的对应变量。前向的 scope 会作为输出被静态图 pylayer op 传递出去。

`saved_tensor` 作用是获取被 `save_for_backward` 保存的中间变量，在动转静中，`saved_tensor` 可以在反向 block 内根据保存的中间变量的变量属性创建对应的 var，然后在运行期根据保存的中间变量的名字从前向的 scope 中获取并赋值给当前 block 的相关变量。

总而言之，可以通过 scope 和算子属性，来实现前向信息和反向信息的传递。



### 课题进展和成果展示

见 PPT



### 下一步工作计划

见 PPT



### 自我总结

在参与这个开源项目需求的过程中，我获得了很多宝贵的经验和教训。首先，通过参与代码贡献，我更加深入地了解了飞桨框架的设计和原理，特别是动态图和静态图的构建和运行机制。这让我对深度学习框架有了更加全面和深入的认识，也让我更加有信心和动力去学习和探索新的技术。

其次，通过与社区其他成员的交流和协作，我学会了如何更好地解决问题和合作开发。在社区中，我结识了很多有才华和热心的开发者，我们一起探讨问题、解决问题，共同为项目的进展贡献力量。这种团队协作的精神和氛围也让我更加有归属感和责任感，更加珍视团队中每一个人的贡献。

在实现这个需求的过程中，我遇到了一些困难和挑战。首先，我需要深入了解飞桨框架的内部实现和PyLayer机制的细节，这对我来说是一个挑战。其次，我需要理解@to_static装饰器内部的细节，这也是一个具有挑战性的任务。然而，通过不断地阅读文档、调试代码和向社区求助，我最终成功地解决了这些问题。

通过这个需求，我不仅学到了技术知识，更重要的是学到了如何解决问题和与他人合作。我相信，在未来的学习和工作中，我会继续将这些知识和经验运用到实践中，为开源社区和自己的成长贡献更多的力量。



### 结语

以上，就是我中期考核答辩自述，敬请您提出宝贵的意见，谢谢！