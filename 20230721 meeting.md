# 0721 meeting

主题：例行周会

时间：2023.07.21 10:05



#### 本周进展

1. pylayer 感知，在 convert_call_func() 函数里返回一个 StaticPylayer，该对象进行 forward 和 backward 函数的转写；并构造了 StaticPylayerContext 对象与 forward 和 backward 进行参数绑定，使 ctx.save_for_backward 与 ctx.saved_tensors 调用StaticPylayerContext 成员函数
2. 仿照 cond op 在 python 侧写了一个上下文管理器用于新增 sub_block 和 与 parent block 进行数据交互



#### 下周计划

1. 调通无 kernel 的 op，使之能执行某个子图
2. 研究反向建图的逻辑，以及写好该无 kernel op 反向执行的逻辑



#### 目前需要改进的

1. 改进 StaticPylayer 的初始化参数，只需要传入 func_self 即可，因为 func_self 包含了该 PyLayer 对象的所有信息



#### 后续需要改进的

1. 感知到 PyLayer 而不是 PyLayer.apply，进而转写后的 PyLayer.apply 最终应该会调用 StaticPyLayer.apply



#### 方案设计探讨

1. 可以研究 backward.py 里 gradient 函数的逻辑。目标是标记 static_pylayer op，由于已经在 apply 的时候构建好了 forward block 和 backward block，所以在反向建图的时候，不需要额外生成一个反向block
2. 前向和反向的信息如何传递，还需进一步探究。目前的想法是 ctx 存储的信息可以通过 scope 传递；对 static pylayer op 进行反向建图的时候，可以”直接复制一份 static pylayer op “（有风险，因为 pylayer 前向和反向的输出输入参数不一样）或者 ”给static pylayer grad op 标记一个属性指向 backward block“
3. 调通整个 PyLayer 动转静的流程后，可以在单测里 PyLayer.apply 的前后插入控制流算子，检查 PyLayer 是否正确



#### 答疑时间

1. RunImpl 的返回值（输出值是如何传递出去的）？
   答：通过 scope 机制
2. paddle.static.nn 模块里有的函数为啥有 `in_dygraph_mode()` 分支？（比如 paddle.static.nn.cond）
   答：为了动态图和静态图编程接口的统一，也就是动静结合
3. cond op 和 while op 如何给分支传递参数？文档里的转写后代码与现在版本的不一样，为什么现在要用 nonlocal 机制？
   答：统一接口，更方便的进行自动代码生成