## PyLayer @to_static 技术实现文档

主题：项目开始前两周初步制定的技术实现文档

时间：2023.07.15



1. 感知 PyLayer

   当前 ast 转写可将 PyLayer 的 apply 调用转写为 `convert_call`, 转写样例如下：

   ```python
   class cus_tanh(PyLayer):
       def forward(ctx, x):
           ....
       def backward(ctx, dy):
           ...
           
   class SimpleNet(paddle.nn.Layer):
       def __init__(self):
           ...
       
       @to_static
       def forward(self, x):
           ...
           out = cus_tanh.apply(x)		# <---- 会被 convert_call 来转写
           ...
           
   #### 转写后：
   out = _jst.Call(cus_tanh.apply)(x)
   ```

   因此只需要在 `python/paddle/jit/dy2static/convert_call_func.py` 对 `convert_call` 函数编写 cus_tanh.apply 对应的转发规则即可。经验证，当传入的 `func` 是 PyLayer 的 `apply` 方法时，可以被如下规则感知到：

   ```python
   if hasattr(func, "__self__") and isinstance(func.__self__, PyLayerMeta):
       print("########### okkkkkkkk ###########")
   ```

   具体地规则书写，可以参考 `convert_call`函数里的判断普通函数的方法，以保证完备性。进一步的，通过 `func.__self__.forward` 和 `func.__self__.backward` 即可访问到 `cus_tanh` 的前向和反向逻辑，可在后续进一步对 `func.__self__.forward` 和 `func.__self__.backward` 进行函数转写，然后生成对应的静态子图

   > 可能遇到的问题：
   >
   > 1. PyLayer 的前向和反向逻辑中包含 `ctx.save_for_backward()` 和 `ctx.saved_tensor()` 的操作。对于这两种操作，同样会被 `convert_call` 感知到。初步的想法是对于这两种操作存储的中间变量，可以存储在 `static_pylayer` OP 的 scope 里
   > 2. 如果 PyLayer 的 forward 和 backward 方法的参数列表中还包含了非 Tensor 对象，或者forward 、 backward 函数调用了其他用户自定义的函数（这些函数里调用了Paddle API），又该如何感知？待完成初期锚点的实现后，还待讨论...



2. 新建一个 `static_pylayer` OP

   这是一个静态图的无 kernel op, 持有两个属性 `forward_block` 和 `backward_block`，backward 插入的 `static_pylayer_grad` OP 持有 `backward_block` 的 block id，在   `static_pylayer_grad` 会调用相应的 backward_block ，op 的写法可以参考 `conditional_block` op 。最终期望生成的 block 如下：

   ```python
   {	// block 0
       ...
        {Out=['pylayer.tmp_0']} = static_pylayer(inputs={X=['eager_tmp_0']}, sub_block = block[1], backward_block = block[2], op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
    	...
    	...
       {Input@GRAD=['eager_tmp_0@GRAD@RENAME@block0@0']} = static_pylayer_block_grad(inputs={Input=['eager_tmp_0'], Out=['pylayer.tmp_0'], Out@GRAD=['pylayer.tmp_0@GRAD'], Scope=['_generated_var_3']}, is_scalar_condition = True, op_device = , op_role = 1, sub_block = block[2])
    	...
   }
   
   {	// block 1, 对应于 cus_tanh 的 forward 逻辑
       ...
        {Out=['tanh.tmp_0']} = tanh(inputs={X=['eager_tmp_0']}, sub_block = block[1], backward_block = block[2], op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
         {Out=['pylayer.tmp_0']} = assign(inputs={X=['tanh.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
    	...
   }
   
   {	// block 2, 对应于 cus_tanh 的 backward 逻辑
       ...
        {Out=['square.tmp_0']} = square(inputs={X=['scope.tanh.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
        {Out=['sub.tmp_0']} = scale(inputs={ScaleTensor=[], X=['square.tmp_0']}, bias = 1.0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = True, scale = -1.0, with_quant_attr = False)
        {Out=['elementwise_mul.tmp_0']} = elementwise_mul(inputs={X=['sub.tmp_0'], Y=['square.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
    	 {Out=['eager_tmp_0@GRAD@RENAME@block0@0']} = assign(inputs={X=['elementwise_mul.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = -1, reduce_all = True, with_quant_attr = False)
    	...
   }
   
   ```

   > 可能遇到的问题：
   >
   > 1. 如果 PyLayer 的 forward 和 backward 方法的参数列表中还包含了非 Tensor 对象，或者forward 、 backward 函数调用了其他用户自定义的函数（这些函数里调用了Paddle API），block 又该如何构建？

